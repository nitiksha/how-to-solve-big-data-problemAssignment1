2. Problem Statement
Explain the following terms in detail:
1. Various sources of Big Data
2. 3 V's of Big Data
3. Horizontal Scaling and Vertical Scaling
4. Need and Working of Hadoop


Ques 1. Various sources of Big Data
Ans 1. Varieties of data generated by machines, people, and organizations. 
With machine generated data we refer to data generated from real time sensors in industrial machinery or vehicles
that logs that track user behavior online, environmental sensors or personal health trackers, and many other
sense data resources. Some sources are:
1)Banking:Large chunk of data is produced with each transaction and other entries.
2)Social network profiles:Facebook, LinkedIn, Yahoo, Google, and specific-interest social or travel sites
3)Social influencers-Editor, analyst and subject-matter expert blog comments, user forums, Twitter & Facebook likes.
4)Activity-generated data-computer and mobile devices logs,webs site tracking info.and application logs
5)Software as a Service (SaaS) and cloud applications
6)Public—Microsoft Azure MarketPlace/DataMarket, The World Bank, SEC/Edgar, Wikipedia, IMDb, etc.
7)Hadoop MapReduce application results.
8)Data warehouse appliances—Teradata, IBM Netezza, EMC Greenplum, etc. are collecting from operational
   systems the internal, transactional data.
9)Columnar/NoSQL data sources—MongoDB, Cassandra, InfoBright, etc. 
10) Network and in-stream monitoring technologies.



Ques 2 ) 3 V's of Big Data
Ans 2.Volume
     Terabytes and Petabytes of the storage system for enterprises.
     We currently see the exponential growth in the data storage as the data is now more than text data.
     We can find data in the format of videos, musics and large images on our social media channels.
     Variety
     90% of the data that is generated by organisation is unstructured data. Data today comes in 
     many different formats: structured data, semi-structured data, unstructured data and even complex
     structured data.
     Velocity
    The Velocity is the speed at which the data is created, stored, analysed and visualized. 
    In the past, when batch processing was common practice, it was normal to receive an update 
    from the database every night or even every week. 
    Computers and servers required substantial time to process the data and update the databases.
    
    
Ques 3)Horizontal Scaling and Vertical Scaling
Ans 3)VERTICAL SCALING
    It refers to the process of adding more physical resources such as memory, storage and 
  CPU to the existing database server for improving the performance.
   Vertical scaling helps in upgrading the cIapacity of the existing database server.
      pros:
      It consumes less power as compared to running multiple servers
     Administrative efforts will be reduced as you need to handle and manage just one system
     Cooling costs are lesser than horizontal scaling
     Reduced software costs
     Implementation isn’t difficult
     The licensing costs are less
     cons:
     There is a greater risk of hardware failure which can cause bigger outages
    Limited scope of upgradeability in the future
    Severe vendor lock-in
    The overall cost of implementing is really expensive
    Horizontal Scaling
    It can also be defined as the ability to increase the capacity by connecting multiple software
    or hardware entities in such a manner that they function as a single logical unit.
    pros:
    The application compatibility is retained
     Much cheaper compared to scaling-up
    Takes advantage of smaller systems
    Easy to upgrade
   Resilience is improved due to the presence of discrete, multiple systems
   Easier to run fault-tolerance
   cons:
   The licensing fees are more
   Utility costs such as cooling and electricity are high
    It has a bigger footprint in the Data Center
    Supporting linear increases in capacity
    
    
    
    
Ques 4)Need and Working of Hadoop
Ans 4)Hadoop is changing the perception of handling Big Data especially the unstructured data. 
     Apache Hadoop software library, which is a framework, plays a vital role in handling Big Data.  
     Apache Hadoop enables surplus data to be streamlined for any distributed processing system across
     clusters of computers using simple programming models. It truly is made to scale up from single servers
     to a large number of machines, each and every offering local computation, and storage space. Instead of 
     depending on hardware to provide high-availability, the library itself is built to detect and handle breakdowns 
     at the application layer,so providing an extremely available service along with a cluster of computers, as both
     versions might be vulnerable to failures.
     Hadoop runs applications using the MapReduce algorithm, where the data is processed in parallel on different CPU nodes.
     In short, Hadoop framework is capable enough to develop applications capable of running on clusters of computers and 
     they could perform complete statistical analysis for a huge amounts of data.
     Hadoop Distributed Filesystem (HDFS)is important part of it.
     Data Processing Framework & MapReduce etc are used in hadoop
